{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902f62a8",
   "metadata": {},
   "source": [
    "# Advanced Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ec67a",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour (KNN)\n",
    "\n",
    "K-Nearest Neighbour is a simple yet powerful **instance-based learning algorithm** used for both classification and regression tasks. Unlike parametric models that learn a function during training, KNN is a **lazy learner** that stores all training data and makes predictions by finding the K closest examples to a new data point.\n",
    "\n",
    "<span style=\"color : red\">Band 5 & 6 students should understand how KNN makes predictions using distance metrics, the impact of the K value, and the differences between KNN classification and regression.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How KNN Works: The Algorithm\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[New Data Point] --> B[Calculate Distance to<br/>All Training Points]\n",
    "    B --> C[Sort Distances<br/>Ascending Order]\n",
    "    C --> D[Select K Nearest<br/>Neighbours]\n",
    "    D --> E{Task Type?}\n",
    "    E -->|Classification| F[Vote: Most Common<br/>Class Among K]\n",
    "    E -->|Regression| G[Average: Mean Value<br/>Among K]\n",
    "    F --> H[Predicted Class]\n",
    "    G --> I[Predicted Value]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style E fill:#ffeaa7,color:#333\n",
    "    style F fill:#d4edda,color:#333\n",
    "    style G fill:#d4edda,color:#333\n",
    "    style H fill:#f8d7da,color:#333\n",
    "    style I fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1. **Store Training Data**: KNN stores all training examples (lazy learning)\n",
    "2. **Calculate Distance**: When a new point arrives, calculate distance to all training points\n",
    "3. **Find K Nearest**: Select the K closest training examples\n",
    "4. **Make Prediction**:\n",
    "   - **Classification**: Majority vote among K neighbours\n",
    "   - **Regression**: Average (mean) of K neighbours' values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the K Value\n",
    "\n",
    "The value of K significantly impacts model performance:\n",
    "\n",
    "| K Value | Behaviour | Advantages | Disadvantages |\n",
    "| --- | --- | --- | --- |\n",
    "| K = 1 | Uses only nearest neighbour | Simple, captures fine patterns | Sensitive to noise, overfitting |\n",
    "| K = 3-5 | Small neighbourhood | Good balance for many problems | May still be affected by noise |\n",
    "| K = 10-20 | Medium neighbourhood | More stable, less noise sensitive | May miss local patterns |\n",
    "| K = Large | Uses many neighbours | Smooth decision boundaries | Underfitting, ignores local structure |\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Small K<br/>e.g., K=1] --> B[Complex Boundaries<br/>High Variance<br/>Overfitting Risk]\n",
    "    C[Medium K<br/>e.g., K=5-10] --> D[Balanced<br/>Good Generalization]\n",
    "    E[Large K<br/>e.g., K=50] --> F[Smooth Boundaries<br/>High Bias<br/>Underfitting Risk]\n",
    "    \n",
    "    style A fill:#f8d7da,color:#333\n",
    "    style C fill:#d4edda,color:#333\n",
    "    style E fill:#fff3cd,color:#333\n",
    "```\n",
    "\n",
    "> [!Note]\n",
    ">\n",
    "> **Best Practice**: Always use an **odd number** for K in binary classification to avoid ties. Use cross-validation to find the optimal K value for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b676a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from utils_common import generate_data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random data set\n",
    "m = generate_data(0, 50, 0, 50, 150, 0.4)\n",
    "n = generate_data(0, 50, 0, 50, 150, 0.45)\n",
    "o = generate_data(30, 50, 30, 50, 10, 0.1)\n",
    "\n",
    "cols = [random.randint(0, 1) for _ in range(10)]\n",
    "radii = [1, 2, 3, 4, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea05144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbour Regression\n",
    "# Mean of the K-Nearest Neighbours is used as the prediction\n",
    "zoom = True\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.scatter(40, 40, color='red', label='Target Point')\n",
    "if not zoom:\n",
    "    plt.scatter(m[0], m[1], color='black')\n",
    "    plt.scatter(n[0], n[1], color='black')\n",
    "else:\n",
    "    plt.scatter(o[0], o[1], color='black')\n",
    "    plt.plot([],[],color='green', linestyle='--', linewidth=1, label='Euclidean Distance')\n",
    "    for xi, yi in zip(o[0], o[1]):\n",
    "        plt.plot([40, xi], [40, yi], color='green', linestyle='--', linewidth=1)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"K-Nearest Neighbour\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565cbde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbour Classification\\\n",
    "# Mode Class of the K-Nearest Neighbours is used as the prediction\n",
    "zoom = True\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(40, 40, color='red', label='Target Point')\n",
    "if not zoom:\n",
    "    plt.scatter(m[0], m[1], color='gold', label='Class 0')\n",
    "    plt.scatter(n[0], n[1], color='indigo', label='Class 1')\n",
    "else:\n",
    "    plt.scatter([], [], c='gold', label='Class 0')\n",
    "    plt.scatter([], [], c='indigo', label='Class 1')\n",
    "    plt.scatter(o[0], o[1], c=cols)\n",
    "    plt.plot([],[],color='green', linestyle='--', linewidth=1, label='Euclidean Distance')\n",
    "    for radius in radii:\n",
    "        circle = Circle((40, 40), radius, color='blue', fill=False, linestyle='--')\n",
    "        ax.add_patch(circle)\n",
    "    for xi, yi in zip(o[0], o[1]):\n",
    "        plt.plot([40, xi], [40, yi], color='green', linestyle='--', linewidth=1)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.title(\"K-Nearest Neighbour Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bafd0d",
   "metadata": {},
   "source": [
    "#### Measuring Distance\n",
    "\n",
    "KNN uses distance metrics to find nearest neighbours. The two most common are:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Distance Metrics] --> B[Euclidean Distance]\n",
    "    A --> C[Manhattan Distance]\n",
    "    \n",
    "    B --> B1[Formula: \u221a[(x\u2082-x\u2081)\u00b2 + (y\u2082-y\u2081)\u00b2]]\n",
    "    B --> B2[Straight-line distance]\n",
    "    B --> B3[Most commonly used]\n",
    "    \n",
    "    C --> C1[Formula: |x\u2082-x\u2081| + |y\u2082-y\u2081|]\n",
    "    C --> C2[Grid-like, right-angle path]\n",
    "    C --> C3[Useful for grid-based problems]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#d4edda,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "```\n",
    "\n",
    "**Euclidean Distance** is the most commonly used measure in KNN, representing the straight-line distance between two points. **Manhattan Distance** measures the distance along axes at right angles, useful when movement is restricted to a grid pattern (like city blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Euclidean & Manhattan Distance\n",
    "A, B = np.array([2, 3]), np.array([8, 7])\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(*A, color='blue', s=100, label='A')\n",
    "ax.scatter(*B, color='red', s=100, label='B')\n",
    "# Euclidean (straight line)\n",
    "ax.plot([A[0], B[0]], [A[1], B[1]], color='green', lw=2, label='Euclidean')\n",
    "# Manhattan (right-angle path)\n",
    "ax.plot([A[0], B[0]], [A[1], A[1]], color='orange', ls='--', lw=2)\n",
    "ax.plot([B[0], B[0]], [A[1], B[1]], color='orange', ls='--', lw=2, label='Manhattan')\n",
    "ax.annotate('A', A + [0.2, -0.2], fontsize=12, color='blue')\n",
    "ax.annotate('B', B + [0.2, 0.2], fontsize=12, color='red')\n",
    "ax.grid(True, linestyle=':')\n",
    "ax.set(xlim=(0, 10), ylim=(0, 10), aspect='equal', xlabel='X', ylabel='Y', title='Euclidean vs Manhattan Distance')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN: Advantages and Disadvantages\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| --- | --- |\n",
    "| Simple and intuitive algorithm | Computationally expensive at prediction time |\n",
    "| No training phase (lazy learning) | Requires storing entire training dataset |\n",
    "| Works well with multi-class problems | Sensitive to irrelevant features |\n",
    "| No assumptions about data distribution | Performance degrades with high dimensions (curse of dimensionality) |\n",
    "| Naturally handles multi-class classification | Sensitive to scale (requires feature normalization) |\n",
    "| Can capture complex decision boundaries | Needs optimal K value selection |\n",
    "\n",
    "> [!Important]\n",
    ">\n",
    "> **Feature Scaling is Critical**: KNN is distance-based, so features with larger scales will dominate the distance calculation. Always normalize or standardize features before using KNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b81c1",
   "metadata": {},
   "source": [
    "### Neural Network Course Specifications\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"images\\NN_Course-Specs.png\" alt=\"Course Specs Neural Network image\" width=\"500\" />\n",
    "    <figcaption><p><em>Source: Page 29 of the Software Engineering Course Specifications</em></p>\n",
    "    </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Neural networks were designed to mimic the processing inside the human brain. They consist of a series of interconnected nodes (artificial neurones). Each neurone can accept a binary input signal and potentially output another signal to connected nodes.\n",
    "\n",
    "#### Training cycle\n",
    "\n",
    "Internal weightings and threshold values for each node are determined in the initial training cycle for each neural network. The system is exposed to a series of inputs with known responses. Linear regression with backward chaining is used to iteratively determine the set of unique values required for output. Regular exposure to the training cycle results in improved accuracy and pattern matching.\n",
    "\n",
    "#### Execution cycle\n",
    "\n",
    "In the diagram, signal strength between nodes with the strongest weightings are thicker representing a higher priority in determining the final output. The execution cycle follows the training cycle and utilises the internal values developed during the training cycle to determine the output.\n",
    "\n",
    "Page 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776f489",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision Trees are a powerful and interpretable supervised learning algorithm that creates a tree-like model of decisions. They work by recursively splitting the data based on feature values, creating a hierarchical structure that mimics human decision-making processes.\n",
    "\n",
    "<span style=\"color : red\">Band 5 & 6 students should understand how decision trees split data, the concepts of entropy and information gain, and how to interpret tree structures.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Structure\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Root Node<br/>Feature: Age<br/>Age \u2264 30?] -->|Yes| B[Internal Node<br/>Feature: Income<br/>Income \u2264 50K?]\n",
    "    A -->|No| C[Internal Node<br/>Feature: Credit Score<br/>Score \u2264 650?]\n",
    "    \n",
    "    B -->|Yes| D[Leaf Node<br/>Prediction: Deny]\n",
    "    B -->|No| E[Leaf Node<br/>Prediction: Approve]\n",
    "    \n",
    "    C -->|Yes| F[Leaf Node<br/>Prediction: Deny]\n",
    "    C -->|No| G[Leaf Node<br/>Prediction: Approve]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#f8d7da,color:#333\n",
    "    style E fill:#d4edda,color:#333\n",
    "    style F fill:#f8d7da,color:#333\n",
    "    style G fill:#d4edda,color:#333\n",
    "```\n",
    "\n",
    "**Tree Components:**\n",
    "\n",
    "- **Root Node**: Starting point, contains entire dataset\n",
    "- **Internal Nodes**: Decision points based on feature tests\n",
    "- **Branches**: Represent outcomes of tests (Yes/No, or feature ranges)\n",
    "- **Leaf Nodes**: Terminal nodes containing predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Decision Trees Build: The Algorithm\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Start with<br/>Entire Dataset] --> B[Calculate Impurity<br/>Current Node]\n",
    "    B --> C[For Each Feature]\n",
    "    C --> D[Try All Possible<br/>Split Points]\n",
    "    D --> E[Calculate Information Gain<br/>or Gini Decrease]\n",
    "    E --> F[Select Best Split<br/>Highest Info Gain]\n",
    "    F --> G[Split Data into<br/>Left & Right Subsets]\n",
    "    G --> H{Stopping<br/>Criterion Met?}\n",
    "    H -->|No| I[Recursively Build<br/>Subtrees]\n",
    "    I --> B\n",
    "    H -->|Yes| J[Create Leaf Node<br/>with Prediction]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style F fill:#d4edda,color:#333\n",
    "    style H fill:#ffeaa7,color:#333\n",
    "    style J fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1. **Root Node**: The algorithm starts with the entire dataset\n",
    "2. **Splitting**: At each node, select the feature and split value that best separates the data, minimizing impurity (entropy or Gini)\n",
    "3. **Recursion**: Repeat the process for each subset, creating new nodes and branches\n",
    "4. **Stopping Criteria**: Stop when:\n",
    "   - Maximum tree depth reached\n",
    "   - Minimum samples per leaf reached\n",
    "   - No further information gain possible\n",
    "   - All samples in node belong to same class\n",
    "5. **Leaf Nodes**: Terminal nodes contain the final predictions\n",
    "6. **Prediction**: Follow the path from root to leaf based on feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain and Impurity Measures\n",
    "\n",
    "Decision trees use **impurity measures** to determine the best splits:\n",
    "\n",
    "| Measure | Formula | Use Case | Range |\n",
    "| --- | --- | --- | --- |\n",
    "| **Entropy** | $-\\sum p_i \\log_2(p_i)$ | ID3, C4.5 algorithms | 0 (pure) to 1 (impure) |\n",
    "| **Gini Impurity** | $1 - \\sum p_i^2$ | CART algorithm (scikit-learn default) | 0 (pure) to 0.5 (impure) |\n",
    "| **Variance** | $\\frac{1}{n}\\sum(y_i - \\bar{y})^2$ | Regression trees | Varies by data |\n",
    "\n",
    "**Information Gain** = Impurity(parent) - Weighted Average of Impurity(children)\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[High Entropy<br/>Mixed Classes<br/>Impure] --> B[Split on<br/>Best Feature]\n",
    "    B --> C[Lower Entropy<br/>More Homogeneous<br/>Purer]\n",
    "    C --> D[Repeat Until<br/>Pure Leaf Nodes]\n",
    "    \n",
    "    style A fill:#f8d7da,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#d4edda,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "```\n",
    "\n",
    "> [!Note]\n",
    ">\n",
    "> The goal is to maximize **Information Gain** at each split, creating subsets that are as pure (homogeneous) as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees: Advantages and Disadvantages\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| --- | --- |\n",
    "| Easy to understand and interpret (white-box model) | Prone to overfitting, especially with deep trees |\n",
    "| Requires little data preparation (no scaling needed) | Can be unstable (small data changes \u2192 different tree) |\n",
    "| Handles both numerical and categorical data | Biased toward features with more levels |\n",
    "| Can model non-linear relationships | Greedy algorithm (may not find global optimum) |\n",
    "| Works with missing values (some implementations) | Can create over-complex trees (poor generalization) |\n",
    "| Automatic feature selection through splits | Difficulty capturing XOR and other complex patterns |\n",
    "\n",
    "> [!Important]\n",
    ">\n",
    "> **Overfitting Prevention**: Use techniques like pruning, setting max_depth, min_samples_split, and min_samples_leaf to prevent overfitting. Consider using **Random Forests** (ensemble of decision trees) for better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing KNN and Decision Trees\n",
    "\n",
    "| Aspect | K-Nearest Neighbour | Decision Trees |\n",
    "| --- | --- | --- |\n",
    "| **Learning Type** | Lazy (instance-based) | Eager (model-based) |\n",
    "| **Training Time** | Fast (just stores data) | Slower (builds tree) |\n",
    "| **Prediction Time** | Slow (calculates all distances) | Fast (traverses tree) |\n",
    "| **Interpretability** | Low (black box) | High (visual tree) |\n",
    "| **Feature Scaling** | Required | Not required |\n",
    "| **Handling Non-linearity** | Good | Excellent |\n",
    "| **Memory Usage** | High (stores all data) | Low (stores tree structure) |\n",
    "| **Overfitting Risk** | Low K \u2192 High risk | Deep trees \u2192 High risk |\n",
    "| **Best Use Case** | Small datasets, Pattern recognition | Interpretability needed, Mixed data types |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}